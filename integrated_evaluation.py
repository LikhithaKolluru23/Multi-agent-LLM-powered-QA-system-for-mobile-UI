
"""
Android in the Wild Video Evaluation System with MockAndroid
1. Loads 3-5 real Android in the Wild videos
2. Extracts task prompts from video analysis
3. Runs multi-agent system in MockAndroidEnv
4. Compares agent trace vs ground truth video actions
5. Scores accuracy, robustness, and generalization
"""

import time
import json
import numpy as np
import cv2
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import enum
import tensorflow as tf
import tensorflow_datasets as tfds
from pathlib import Path

# Action types from Android in the Wild
class ActionType(enum.IntEnum):
    TYPE = 3
    DUAL_POINT = 4
    PRESS_BACK = 5
    PRESS_HOME = 6
    PRESS_ENTER = 7
    SCROLL = 8
    STATUS_TASK_COMPLETE = 10
    STATUS_TASK_IMPOSSIBLE = 11

@dataclass
class VideoFrame:
    """Single frame from Android in the Wild video"""
    frame_id: int
    timestamp: float
    image: np.ndarray
    ui_elements: List[Dict[str, Any]]
    action: Optional[Dict[str, Any]] = None

@dataclass
class AndroidInWildVideo:
    """Real video episode from Android in the Wild dataset"""
    episode_id: str
    goal_info: str
    frames: List[VideoFrame]
    actions: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    video_path: str

@dataclass
class AgentVideoTrace:
    """Agent execution trace with video frames"""
    episode_id: str
    goal: str
    steps: List[Dict[str, Any]]
    duration: float
    success: bool
    final_state: Dict[str, Any]

@dataclass
class VideoComparisonResult:
    """Result of comparing agent trace vs ground truth video"""
    episode_id: str
    task_prompt: str
    real_goal: str
    accuracy_score: float
    robustness_score: float
    generalization_score: float
    agent_trace: AgentVideoTrace
    ground_truth_actions: List[Dict[str, Any]]
    detailed_analysis: Dict[str, Any]

class AndroidInWildVideoLoader:
    """Loads and processes Android in the Wild video data"""
    
    def __init__(self, data_dir: str = r"C:\Users\likhi\Downloads\google_apps"):
        self.data_dir = Path(data_dir)
        if not self.data_dir.exists():
            print(f" Dataset directory not found: {data_dir}")
            self.data_dir = Path("./android_in_wild_data")
            self.data_dir.mkdir(exist_ok=True)
        else:
            print(f" Found Google Apps dataset directory: {data_dir}")
            # Test the pattern
            pattern = str(self.data_dir / "*")
            files = tf.io.gfile.glob(pattern)
            print(f" Found {len(files)} files in dataset directory")
    
    def load_videos(self, num_videos: int = 5) -> List[AndroidInWildVideo]:
        """Load real Android in the Wild videos"""
        
        print(f" Loading {num_videos} Android in the Wild videos...")
        
        # Load from Google Apps dataset using the working method
        if "google_apps" in str(self.data_dir):
            try:
                videos = self._load_google_apps_dataset(num_videos)
                if videos:
                    print(f" Successfully loaded {len(videos)} REAL videos from Google Apps dataset")
                    return videos
            except Exception as e:
                print(f" Google Apps dataset loading failed: {e}")
        
        # Fallback to mock data only if real loading fails
        print(" Falling back to mock data...")
        return self._create_realistic_mock_videos(num_videos)
    
    def _load_google_apps_dataset(self, num_videos: int) -> List[AndroidInWildVideo]:
        """Load from Google Apps dataset using the working method"""
        
        print(f" Loading from dataset: {self.data_dir}")
        
        # Use tf.io.gfile to find files
        pattern = str(self.data_dir / "*")
        filenames = tf.io.gfile.glob(pattern)
        
        if not filenames:
            print(f" No files found with pattern: {pattern}")
            return []
        
        print(f" Found {len(filenames)} dataset files")
        
        # Create TFRecord dataset
        dataset = tf.data.TFRecordDataset(filenames, compression_type='GZIP')
        dataset_iterator = dataset.as_numpy_iterator()
        
        videos = []
        
        for video_idx in range(num_videos):
            try:
                episode = self._get_episode(dataset_iterator)
                
                if not episode:
                    print(f"ðŸ“„ No more episodes available")
                    break
                
                video = self._convert_episode_to_video(episode, video_idx)
                videos.append(video)
                
                print(f" Loaded video {video_idx + 1}: '{video.goal_info}' ({len(video.frames)} frames)")
                
            except Exception as e:
                print(f" Error loading video {video_idx + 1}: {e}")
                break
        
        return videos
    
    def _get_episode(self, dataset_iterator):
        """Extract one complete episode (from working test)"""
        episode = []
        episode_id = None
        
        try:
            for d in dataset_iterator:
                ex = tf.train.Example()
                ex.ParseFromString(d)
                
                ep_id = ex.features.feature['episode_id'].bytes_list.value[0].decode('utf-8')
                
                if episode_id is None:
                    episode_id = ep_id
                    episode.append(ex)
                elif ep_id == episode_id:
                    episode.append(ex)
                else:
                    # New episode started
                    break
                    
        except StopIteration:
            pass
        except Exception as e:
            print(f" Error extracting episode: {e}")
        
        return episode
    
    def _convert_episode_to_video(self, episode: List, video_id: int) -> AndroidInWildVideo:
        """Convert TensorFlow episode to AndroidInWildVideo format"""
        
        if not episode:
            raise ValueError("Empty episode")
        
        # Extract basic info
        first_frame = episode[0]
        features = first_frame.features.feature
        
        goal_info = features['goal_info'].bytes_list.value[0].decode('utf-8')
        episode_id = features['episode_id'].bytes_list.value[0].decode('utf-8')
        
        frames = []
        actions = []
        
        for frame_idx, ex in enumerate(episode):
            try:
                # Extract image info
                image_height = ex.features.feature['image/height'].int64_list.value[0]
                image_width = ex.features.feature['image/width'].int64_list.value[0]
                image_channels = ex.features.feature['image/channels'].int64_list.value[0]
                
                # Decode image
                image_data = ex.features.feature['image/encoded'].bytes_list.value[0]
                image = tf.io.decode_raw(image_data, out_type=tf.uint8)
                image = tf.reshape(image, (image_height, image_width, image_channels)).numpy()
                
                # Extract UI elements
                ui_elements = []
                if 'image/ui_annotations_positions' in ex.features.feature:
                    try:
                        positions = self._get_annotation_positions(ex, image_height, image_width)
                        for pos in positions:
                            ui_elements.append({
                                "bounds": pos.tolist(),
                                "type": "ui_element"
                            })
                    except:
                        pass
                
                # Extract action
                action = None
                try:
                    if 'results/yx_touch' in ex.features.feature:
                        touch_y, touch_x = ex.features.feature['results/yx_touch'].float_list.value
                        lift_y, lift_x = ex.features.feature['results/yx_lift'].float_list.value
                        action_type = ex.features.feature['results/action_type'].int64_list.value[0]
                        type_text = ex.features.feature['results/type_action'].bytes_list.value[0].decode('utf-8')
                        
                        action = {
                            "action_type": ActionType(action_type),
                            "touch_point": [int(touch_x * image_width), int(touch_y * image_height)],
                            "lift_point": [int(lift_x * image_width), int(lift_y * image_height)],
                            "timestamp": frame_idx * 1.0,
                            "type_text": type_text
                        }
                        actions.append(action)
                        
                except:
                    pass
                
                # Create video frame
                video_frame = VideoFrame(
                    frame_id=frame_idx,
                    timestamp=frame_idx * 1.0,
                    image=image,
                    ui_elements=ui_elements,
                    action=action
                )
                frames.append(video_frame)
                
            except Exception as e:
                print(f" Error processing frame {frame_idx}: {e}")
                continue
        
        return AndroidInWildVideo(
            episode_id=episode_id,
            goal_info=goal_info,
            frames=frames,
            actions=actions,
            metadata={
                "source": "google_apps_real",
                "image_height": image_height,
                "image_width": image_width,
                "num_frames": len(frames),
                "num_actions": len(actions)
            },
            video_path=str(self.data_dir)
        )
    
    def _get_annotation_positions(self, example, image_height, image_width):
        """Extract UI annotation positions"""
        flattened_positions = np.array(
            example.features.feature['image/ui_annotations_positions'].float_list.value
        )
        positions = np.reshape(flattened_positions, (-1, 4)) * [
            image_height, image_width, image_height, image_width
        ]
        return positions.astype(int)
    
    # Keep all your existing methods like _create_realistic_mock_videos, etc.

# Reuse the MockAndroidEnv and MockMultiAgentQA from the original code
class MockAndroidEnv:
    """Enhanced mock Android environment for video evaluation"""
    
    def __init__(self):
        self.current_screen = "home"
        self.state = {
            "wifi_enabled": False,
            "bluetooth_enabled": False,
            "location_accuracy": True,
            "current_app": "home",
            "screen_stack": ["home"],
            "alarm_set": False,
            "alarm_time": None
        }
        self.action_log = []
        self.screenshot_sequence = []
        
    def reset(self):
        """Reset to home screen"""
        self.current_screen = "home"
        self.state = {
            "wifi_enabled": False,
            "bluetooth_enabled": False,
            "location_accuracy": True,
            "current_app": "home",
            "screen_stack": ["home"],
            "alarm_set": False,
            "alarm_time": None
        }
        self.action_log = []
        self.screenshot_sequence = []
        return self._get_observation()
    
    def step(self, action: Dict[str, Any]):
        """Execute action and return new state with screenshot"""
        self.action_log.append(action)
        
        action_type = action.get("action_type", "")
        
        if action_type == "key" and action.get("key") == "KEYCODE_HOME":
            self._go_home()
        elif action_type == "key" and action.get("key") == "KEYCODE_SETTINGS":
            self._open_settings()
        elif action_type == "tap":
            self._handle_tap(action.get("coordinate", [0, 0]))
        elif action_type == "scroll":
            self._handle_scroll()
        
        obs = self._get_observation()
        
        # Capture mock screenshot
        screenshot = self._capture_screenshot()
        self.screenshot_sequence.append(screenshot)
        
        reward = 1.0 if self._task_completed() else 0.0
        done = self._task_completed()
        info = {"current_screen": self.current_screen, "screenshot": screenshot}
        
        return obs, reward, done, info
    
    def _capture_screenshot(self) -> np.ndarray:
        """Capture mock screenshot of current state"""
        # Generate realistic screenshot based on current screen
        screenshot = np.zeros((1920, 1080, 3), dtype=np.uint8)
        
        # Different colors for different screens
        screen_colors = {
            "home": [50, 50, 50],
            "settings_main": [245, 245, 245],
            "wifi_settings": [240, 240, 240],
            "bluetooth_settings": [240, 240, 240],
            "location_settings": [245, 245, 245],
            "location_accuracy": [240, 240, 240],
            "about_phone": [245, 245, 245],
            "clock_main": [255, 255, 255],
            "alarm_list": [250, 250, 250]
        }
        
        color = screen_colors.get(self.current_screen, [200, 200, 200])
        screenshot[:, :] = color
        
        return screenshot
    
    def _go_home(self):
        """Navigate to home screen"""
        self.current_screen = "home"
        self.state["current_app"] = "home"
        self.state["screen_stack"] = ["home"]
    
    def _open_settings(self):
        """Open Settings app"""
        self.current_screen = "settings_main"
        self.state["current_app"] = "settings"
        self.state["screen_stack"].append("settings_main")
    
    def _handle_tap(self, coordinate: List[int]):
        """Handle tap action with enhanced logic"""
        x, y = coordinate
        
        if self.current_screen == "home":
            if 200 <= x <= 400 and 300 <= y <= 450:  # Clock app
                self.current_screen = "clock_main"
                self.state["current_app"] = "clock"
                self.state["screen_stack"].append("clock_main")
        
        elif self.current_screen == "clock_main":
            if 150 <= y <= 200:  # Alarm tab
                self.current_screen = "alarm_list"
                self.state["screen_stack"].append("alarm_list")
        
        elif self.current_screen == "alarm_list":
            if 850 <= x <= 950 and 100 <= y <= 200:  # Add alarm button
                self.current_screen = "time_picker"
                self.state["screen_stack"].append("time_picker")
        
        elif self.current_screen == "time_picker":
            if 600 <= x <= 800 and 700 <= y <= 900:  # Save button
                self.state["alarm_set"] = True
                self.state["alarm_time"] = "7:00 AM"
                self.current_screen = "alarm_list"
                self.state["screen_stack"].pop()
        
        elif self.current_screen == "settings_main":
            if 400 <= y <= 500:  # WiFi area
                self.current_screen = "wifi_settings"
                self.state["screen_stack"].append("wifi_settings")
            elif 450 <= y <= 550:  # Bluetooth area
                self.current_screen = "bluetooth_settings"
                self.state["screen_stack"].append("bluetooth_settings")
            elif 550 <= y <= 650:  # Location area
                self.current_screen = "location_settings"
                self.state["screen_stack"].append("location_settings")
            elif 1600 <= y <= 1700:  # About phone area
                self.current_screen = "about_phone"
                self.state["screen_stack"].append("about_phone")
        
        elif self.current_screen == "wifi_settings":
            if 900 <= x <= 1000:  # Toggle area
                self.state["wifi_enabled"] = not self.state["wifi_enabled"]
        
        elif self.current_screen == "bluetooth_settings":
            if 900 <= x <= 1000:  # Toggle area
                self.state["bluetooth_enabled"] = not self.state["bluetooth_enabled"]
        
        elif self.current_screen == "location_settings":
            if 400 <= y <= 600:  # Location accuracy area
                self.current_screen = "location_accuracy"
                self.state["screen_stack"].append("location_accuracy")
        
        elif self.current_screen == "location_accuracy":
            if 900 <= x <= 1000:  # Toggle area
                self.state["location_accuracy"] = not self.state["location_accuracy"]
    
    def _handle_scroll(self):
        """Handle scroll action"""
        # Enhanced scroll handling
        if self.current_screen == "settings_main":
            # Scrolling reveals more options like About phone
            pass
    
    def _get_observation(self):
        """Get current observation"""
        return {
            "screen": self.current_screen,
            "state": self.state.copy(),
            "ui_elements": self._get_ui_elements()
        }
    
    def _get_ui_elements(self):
        """Get UI elements for current screen"""
        elements = []
        
        if self.current_screen == "home":
            elements = [
                {"text": "Settings", "bounds": [400, 300, 600, 450], "type": "app_icon"},
                {"text": "Clock", "bounds": [200, 300, 400, 450], "type": "app_icon"}
            ]
        
        elif self.current_screen == "clock_main":
            elements = [
                {"text": "Alarm", "bounds": [150, 150, 250, 200], "type": "tab"},
                {"text": "Timer", "bounds": [300, 150, 400, 200], "type": "tab"}
            ]
        
        elif self.current_screen == "alarm_list":
            elements = [
                {"text": "Alarms", "bounds": [100, 100, 300, 150], "type": "title"},
                {"text": "+", "bounds": [850, 100, 950, 200], "type": "add_button"}
            ]
            if self.state["alarm_set"]:
                elements.append({
                    "text": self.state["alarm_time"], 
                    "bounds": [100, 300, 500, 400], 
                    "type": "alarm_item"
                })
        
        elif self.current_screen == "time_picker":
            elements = [
                {"text": "Set time", "bounds": [100, 200, 300, 250], "type": "title"},
                {"text": "7", "bounds": [300, 350, 400, 450], "type": "hour"},
                {"text": "00", "bounds": [450, 350, 550, 450], "type": "minute"},
                {"text": "AM", "bounds": [600, 350, 700, 450], "type": "ampm"},
                {"text": "Save", "bounds": [600, 700, 800, 900], "type": "save_button"}
            ]
        
        elif self.current_screen == "settings_main":
            elements = [
                {"text": "WiFi", "bounds": [100, 400, 800, 500], "type": "setting_item"},
                {"text": "Bluetooth", "bounds": [100, 450, 800, 550], "type": "setting_item"},
                {"text": "Location", "bounds": [100, 550, 800, 650], "type": "setting_item"},
                {"text": "About phone", "bounds": [100, 1600, 800, 1700], "type": "setting_item"}
            ]
        
        elif self.current_screen == "wifi_settings":
            elements = [
                {"text": "WiFi", "bounds": [100, 300, 400, 400], "type": "title"},
                {"text": "Toggle", "bounds": [900, 350, 1000, 450], "type": "switch", 
                 "enabled": self.state["wifi_enabled"]}
            ]
        
        elif self.current_screen == "bluetooth_settings":
            elements = [
                {"text": "Bluetooth", "bounds": [100, 300, 400, 400], "type": "title"},
                {"text": "Toggle", "bounds": [900, 350, 1000, 450], "type": "switch",
                 "enabled": self.state["bluetooth_enabled"]}
            ]
        
        elif self.current_screen == "location_settings":
            elements = [
                {"text": "Location", "bounds": [100, 200, 400, 300], "type": "title"},
                {"text": "Improve location accuracy", "bounds": [100, 400, 800, 600], "type": "setting_item"}
            ]
        
        elif self.current_screen == "location_accuracy":
            elements = [
                {"text": "Improve location accuracy", "bounds": [100, 200, 600, 300], "type": "title"},
                {"text": "Toggle", "bounds": [900, 350, 1000, 450], "type": "switch",
                 "enabled": self.state["location_accuracy"]}
            ]
        
        elif self.current_screen == "about_phone":
            elements = [
                {"text": "About phone", "bounds": [100, 200, 400, 300], "type": "title"},
                {"text": "Android version", "bounds": [100, 400, 600, 500], "type": "info_item"},
                {"text": "13", "bounds": [700, 400, 800, 500], "type": "info_value"}
            ]
        
        return elements
    
    def _task_completed(self):
        """Check if current task is completed"""
        # This would be set by the specific task being tested
        return False

class VideoMultiAgentQA:
    """Enhanced multi-agent QA system for video evaluation"""
    
    def __init__(self, env: MockAndroidEnv):
        self.env = env
        
    def run_test(self, task_prompt: str) -> AgentVideoTrace:
        """Run test and return agent video trace"""
        
        print(f"ðŸ¤– Running video agent test: {task_prompt}")
        
        start_time = time.time()
        self.env.reset()
        
        # Generate plan based on task
        plan = self._generate_plan(task_prompt)
        
        # Execute plan and capture video frames
        steps = []
        success = False
        
        for i, step_description in enumerate(plan, 1):
            print(f"   Step {i}: {step_description}")
            
            # Determine action from step description
            action = self._step_to_action(step_description)
            
            # Execute action
            obs, reward, done, info = self.env.step(action)
            
            step_result = {
                "step": i,
                "description": step_description,
                "action": action,
                "success": reward > 0 or self._check_step_success(step_description, obs),
                "observation": obs,
                "screenshot": info.get("screenshot")
            }
            steps.append(step_result)
            
            if done or self._task_completed(task_prompt, obs):
                success = True
                break
        
        duration = time.time() - start_time
        
        return AgentVideoTrace(
            episode_id=f"agent_video_test_{int(time.time())}",
            goal=task_prompt,
            steps=steps,
            duration=duration,
            success=success,
            final_state=self.env.state.copy()
        )
    
    def _generate_plan(self, task_prompt: str) -> List[str]:
        """Generate execution plan for task"""
        
        task_lower = task_prompt.lower()
        if 'javascript' in task_lower and 'chrome' in task_lower:
            return [
                "Open Chrome app",
                "Open Chrome menu (3 dots)",
                "Navigate to Settings",
                "Navigate to Site settings", 
                "Navigate to JavaScript",
                "Toggle JavaScript setting"
            ]
        
        elif 'data saver' in task_lower and 'chrome' in task_lower:
            return [
                "Open Chrome app",
                "Open Chrome menu (3 dots)",
                "Navigate to Settings",
                "Navigate to Data Saver",
                "Toggle Data Saver setting"
            ]
        
        elif 'wifi' in task_lower:
            return [
                "Open Settings app",
                "Navigate to WiFi settings",
                "Toggle WiFi setting"
            ]
        if 'wifi' in task_lower:
            return [
                "Open Settings app",
                "Navigate to WiFi settings",
                "Toggle WiFi setting"
            ]
        
        elif 'bluetooth' in task_lower:
            if 'connect' in task_lower:
                return [
                    "Open Settings app",
                    "Navigate to Bluetooth settings", 
                    "Toggle Bluetooth on",
                    "Scan for devices",
                    "Connect to device"
                ]
            else:
                return [
                    "Open Settings app",
                    "Navigate to Bluetooth settings",
                    "Toggle Bluetooth setting"
                ]
        
        elif 'alarm' in task_lower:
            return [
                "Open Clock app",
                "Navigate to Alarm tab",
                "Tap add alarm button",
                "Set time to 7 AM",
                "Save alarm"
            ]
        
        elif 'location' in task_lower and 'accuracy' in task_lower:
            return [
                "Open Settings app",
                "Navigate to Location settings",
                "Open location accuracy settings",
                "Toggle improve location accuracy"
            ]
        
        elif 'android version' in task_lower or 'version' in task_lower:
            return [
                "Open Settings app",
                "Scroll to About phone",
                "Tap About phone",
                "Find Android version"
            ]
        
        else:
            return [
                "Open relevant app",
                "Navigate to target",
                "Complete action"
            ]
    
    def _step_to_action(self, step_description: str) -> Dict[str, Any]:
        """Convert step description to action"""
        
        step_lower = step_description.lower()
        
        if 'open settings' in step_lower:
            return {"action_type": "key", "key": "KEYCODE_SETTINGS"}
        
        elif 'open clock' in step_lower:
            return {"action_type": "tap", "coordinate": [300, 350]}
        
        elif 'alarm tab' in step_lower:
            return {"action_type": "tap", "coordinate": [200, 175]}
        
        elif 'add alarm' in step_lower:
            return {"action_type": "tap", "coordinate": [900, 150]}
        
        elif 'set time' in step_lower:
            return {"action_type": "tap", "coordinate": [400, 400]}
        
        elif 'save alarm' in step_lower:
            return {"action_type": "tap", "coordinate": [700, 800]}
        
        elif 'wifi' in step_lower:
            if 'navigate' in step_lower:
                return {"action_type": "tap", "coordinate": [450, 450]}
            else:  # toggle
                return {"action_type": "tap", "coordinate": [950, 400]}
        
        elif 'bluetooth' in step_lower:
            if 'navigate' in step_lower:
                return {"action_type": "tap", "coordinate": [450, 500]}
            elif 'scan' in step_lower:
                return {"action_type": "tap", "coordinate": [450, 600]}
            elif 'connect' in step_lower:
                return {"action_type": "tap", "coordinate": [600, 700]}
            else:  # toggle
                return {"action_type": "tap", "coordinate": [950, 400]}
        
        elif 'location' in step_lower:
            if 'navigate' in step_lower:
                return {"action_type": "tap", "coordinate": [450, 600]}
            elif 'accuracy' in step_lower:
                return {"action_type": "tap", "coordinate": [450, 500]}
            else:  # toggle
                return {"action_type": "tap", "coordinate": [950, 400]}
        
        elif 'about phone' in step_lower:
            if 'scroll' in step_lower:
                return {"action_type": "scroll", "coordinate": [540, 1200], "direction": "down"}
            else:
                return {"action_type": "tap", "coordinate": [450, 1650]}
        
        elif 'toggle' in step_lower:
            return {"action_type": "tap", "coordinate": [950, 400]}
        
        else:
            return {"action_type": "tap", "coordinate": [540, 1200]}
    
    def _check_step_success(self, step_description: str, observation: Dict) -> bool:
        """Check if step was successful"""
        
        step_lower = step_description.lower()
        current_screen = observation.get("screen", "")
        state = observation.get("state", {})
        
        if 'open settings' in step_lower:
            return current_screen == "settings_main"
        
        elif 'open clock' in step_lower:
            return current_screen == "clock_main"
        
        elif 'alarm tab' in step_lower:
            return current_screen == "alarm_list"
        
        elif 'add alarm' in step_lower:
            return current_screen == "time_picker"
        
        elif 'save alarm' in step_lower:
            return state.get("alarm_set", False)
        
        elif 'wifi' in step_lower and 'navigate' in step_lower:
            return current_screen == "wifi_settings"
        
        elif 'bluetooth' in step_lower and 'navigate' in step_lower:
            return current_screen == "bluetooth_settings"
        
        elif 'location' in step_lower and 'navigate' in step_lower:
            return current_screen == "location_settings"
        
        elif 'accuracy' in step_lower:
            return current_screen == "location_accuracy"
        
        elif 'about phone' in step_lower:
            return current_screen == "about_phone"
        
        else:
            return True  # Default success
    
    def _task_completed(self, task_prompt: str, observation: Dict) -> bool:
        """Check if overall task is completed"""
        
        task_lower = task_prompt.lower()
        state = observation.get("state", {})
        current_screen = observation.get("screen", "")
        
        if 'turn on wifi' in task_lower:
            return state.get("wifi_enabled", False)
        
        elif 'turn off wifi' in task_lower:
            return not state.get("wifi_enabled", True)
        
        elif 'alarm' in task_lower:
            return state.get("alarm_set", False)
        
        elif 'turn on bluetooth' in task_lower:
            return state.get("bluetooth_enabled", False)
        
        elif 'turn off' in task_lower and 'location' in task_lower and 'accuracy' in task_lower:
            return not state.get("location_accuracy", True)
        
        elif 'android version' in task_lower:
            return current_screen == "about_phone"
        
        else:
            return False

class VideoGroundTruthComparator:
    """Compare agent video trace against ground truth video"""
    
    @staticmethod
    def compare_video_traces(agent_trace: AgentVideoTrace, ground_truth_video: AndroidInWildVideo) -> Dict[str, Any]:
        """Compare agent execution vs ground truth video"""
        
        # Action sequence comparison
        agent_actions = [step["action"] for step in agent_trace.steps]
        action_similarity = VideoGroundTruthComparator._compare_action_sequences(
            agent_actions, ground_truth_video.actions
        )
        
        # Video frame comparison (simplified)
        visual_similarity = VideoGroundTruthComparator._compare_visual_sequences(
            agent_trace, ground_truth_video
        )
        
        # Efficiency comparison
        efficiency = len(ground_truth_video.actions) / len(agent_actions) if agent_actions else 0
        efficiency = min(1.0, efficiency)  # Cap at 1.0
        
        # Goal achievement comparison
        goal_match = agent_trace.success
        
        # Navigation pattern comparison
        navigation_similarity = VideoGroundTruthComparator._compare_navigation_patterns(
            agent_trace, ground_truth_video
        )
        
        return {
            "action_similarity": action_similarity,
            "visual_similarity": visual_similarity,
            "navigation_similarity": navigation_similarity,
            "efficiency_score": efficiency,
            "goal_achievement": goal_match,
            "step_count_ratio": len(ground_truth_video.actions) / len(agent_actions) if agent_actions else 0,
            "agent_steps": len(agent_actions),
            "real_steps": len(ground_truth_video.actions)
        }
    
    @staticmethod
    def _compare_action_sequences(agent_actions: List[Dict], real_actions: List[Dict]) -> float:
        """Compare action sequences for similarity"""
        
        if not agent_actions or not real_actions:
            return 0.0
        
        # Extract action types and coordinates
        agent_patterns = [VideoGroundTruthComparator._extract_action_pattern(action) for action in agent_actions]
        real_patterns = [VideoGroundTruthComparator._extract_real_action_pattern(action) for action in real_actions]
        
        # Calculate similarity using longest common subsequence
        similarity = VideoGroundTruthComparator._lcs_similarity(agent_patterns, real_patterns)
        
        return similarity
    
    @staticmethod
    def _compare_visual_sequences(agent_trace: AgentVideoTrace, ground_truth_video: AndroidInWildVideo) -> float:
        """Compare visual sequences (simplified)"""
        
        # In real implementation, this would compare actual screenshots
        # For now, we'll use screen state transitions as a proxy
        
        agent_screens = []
        for step in agent_trace.steps:
            if 'observation' in step and 'screen' in step['observation']:
                agent_screens.append(step['observation']['screen'])
        
        # Extract screen sequence from ground truth (would be from video analysis)
        gt_screens = VideoGroundTruthComparator._extract_screen_sequence(ground_truth_video)
        
        if not agent_screens or not gt_screens:
            return 0.0
        
        # Compare screen sequences
        return VideoGroundTruthComparator._lcs_similarity(agent_screens, gt_screens)
    
    @staticmethod
    def _compare_navigation_patterns(agent_trace: AgentVideoTrace, ground_truth_video: AndroidInWildVideo) -> float:
        """Compare navigation patterns"""
        
        # Extract navigation patterns
        agent_nav = VideoGroundTruthComparator._extract_navigation_pattern(agent_trace)
        gt_nav = VideoGroundTruthComparator._extract_gt_navigation_pattern(ground_truth_video)
        
        if not agent_nav or not gt_nav:
            return 0.0
        
        return VideoGroundTruthComparator._lcs_similarity(agent_nav, gt_nav)
    
    @staticmethod
    def _extract_action_pattern(action: Dict) -> str:
        """Extract action pattern from agent action"""
        action_type = action.get("action_type", "")
        
        if action_type == "tap":
            coord = action.get("coordinate", [0, 0])
            # Discretize coordinates to regions
            region = VideoGroundTruthComparator._coordinate_to_region(coord)
            return f"tap_{region}"
        elif action_type == "key":
            return f"key_{action.get('key', 'unknown')}"
        elif action_type == "scroll":
            return "scroll"
        else:
            return "other"
    
    @staticmethod
    def _extract_real_action_pattern(action: Dict) -> str:
        """Extract action pattern from real action"""
        action_type = action.get("action_type", "")
        
        if action_type == ActionType.DUAL_POINT:
            touch_point = action.get("touch_point", [0, 0])
            region = VideoGroundTruthComparator._coordinate_to_region(touch_point)
            return f"tap_{region}"
        elif action_type == ActionType.SCROLL:
            return "scroll"
        elif action_type in [ActionType.PRESS_BACK, ActionType.PRESS_HOME]:
            return "navigate"
        else:
            return "other"
    
    @staticmethod
    def _coordinate_to_region(coord: List[int]) -> str:
        """Convert coordinate to screen region"""
        x, y = coord
        
        # Define screen regions (for 1080x1920 screen)
        if y < 400:
            return "top"
        elif y < 800:
            return "upper_middle"
        elif y < 1200:
            return "middle"
        elif y < 1600:
            return "lower_middle"
        else:
            return "bottom"
    
    @staticmethod
    def _extract_screen_sequence(ground_truth_video: AndroidInWildVideo) -> List[str]:
        """Extract screen sequence from ground truth video"""
        
        # This would analyze video frames to determine screen states
        # For mock data, use app_sequence from metadata
        if "app_sequence" in ground_truth_video.metadata:
            return ground_truth_video.metadata["app_sequence"]
        
        # Default screen sequence based on goal
        goal = ground_truth_video.goal_info.lower()
        if 'wifi' in goal:
            return ["launcher", "settings", "wifi_settings"]
        elif 'alarm' in goal:
            return ["launcher", "clock", "alarm_list", "time_picker"]
        elif 'bluetooth' in goal:
            return ["launcher", "settings", "bluetooth_settings"]
        elif 'location' in goal:
            return ["launcher", "settings", "location_settings", "location_accuracy"]
        elif 'version' in goal:
            return ["launcher", "settings", "about_phone"]
        else:
            return ["launcher", "settings"]
    
    @staticmethod
    def _extract_navigation_pattern(agent_trace: AgentVideoTrace) -> List[str]:
        """Extract navigation pattern from agent trace"""
        
        pattern = []
        for step in agent_trace.steps:
            action = step.get("action", {})
            action_type = action.get("action_type", "")
            
            if action_type == "key":
                pattern.append("app_switch")
            elif action_type == "tap":
                pattern.append("tap_navigate")
            elif action_type == "scroll":
                pattern.append("scroll")
        
        return pattern
    
    @staticmethod
    def _extract_gt_navigation_pattern(ground_truth_video: AndroidInWildVideo) -> List[str]:
        """Extract navigation pattern from ground truth"""
        
        pattern = []
        for action in ground_truth_video.actions:
            action_type = action.get("action_type", "")
            
            if action_type == ActionType.DUAL_POINT:
                pattern.append("tap_navigate")
            elif action_type == ActionType.SCROLL:
                pattern.append("scroll")
            elif action_type in [ActionType.PRESS_BACK, ActionType.PRESS_HOME]:
                pattern.append("app_switch")
        
        return pattern
    
    @staticmethod
    def _lcs_similarity(seq1: List[str], seq2: List[str]) -> float:
        """Calculate longest common subsequence similarity"""
        
        if not seq1 or not seq2:
            return 0.0
        
        # Dynamic programming LCS
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        similarity = lcs_length / max(m, n)
        
        return similarity
    
class VideoTaskPromptGenerator:
    """Generate task prompts from video analysis"""
    
    @staticmethod
    def generate_task_prompt(video: AndroidInWildVideo) -> str:
        """Generate specific task prompt from video analysis"""
        
        goal = video.goal_info.lower().strip()
        
        # Enhanced goal processing based on video content
        if any(word in goal for word in ['wifi', 'wi-fi', 'wireless']):
            if 'turn on' in goal or 'enable' in goal:
                return "Turn on Wi-Fi"
            elif 'turn off' in goal or 'disable' in goal:
                return "Turn off Wi-Fi"
            else:
                return "Navigate to Wi-Fi settings"
        
        elif 'alarm' in goal:
            if any(time in goal for time in ['7', 'am', 'pm', 'morning']):
                return "Set an alarm for 7 AM"
            else:
                return "Set an alarm"
        
        elif 'bluetooth' in goal:
            if 'connect' in goal or 'pair' in goal:
                return "Enable Bluetooth and connect device"
            elif 'turn on' in goal or 'enable' in goal:
                return "Turn on Bluetooth"
            elif 'turn off' in goal or 'disable' in goal:
                return "Turn off Bluetooth"
            else:
                return "Navigate to Bluetooth settings"
        
        elif any(word in goal for word in ['location', 'accuracy', 'improve']):
            if 'turn off' in goal or 'disable' in goal:
                return "Turn off improve location accuracy"
            else:
                return "Navigate to location accuracy settings"
        
        elif 'android version' in goal or 'version' in goal or 'about' in goal:
            return "Check Android version"
        
        elif 'javascript' in goal:
            if 'turn off' in goal or 'disable' in goal:
                return "Turn off JavaScript in Chrome"
            elif 'turn on' in goal or 'enable' in goal:
                return "Turn on JavaScript in Chrome"
            else:
                return "Manage JavaScript settings in Chrome"
        
        elif 'data saver' in goal:
            if 'turn off' in goal or 'disable' in goal:
                return "Turn off Data Saver in Chrome"
            elif 'turn on' in goal or 'enable' in goal:
                return "Turn on Data Saver in Chrome"
            else:
                return "Manage Data Saver settings in Chrome"
        
        elif 'chrome' in goal and 'settings' in goal:
            return "Navigate to Chrome settings"
        
        elif 'notification' in goal:
            if 'turn off' in goal or 'disable' in goal:
                return "Turn off notifications"
            elif 'turn on' in goal or 'enable' in goal:
                return "Turn on notifications"
            else:
                return "Manage notification settings"
        
        else:
            # Generic task prompt for unknown goals
            return f"Complete task: {goal}"

class AndroidVideoEvaluationSystem:
    """Main video evaluation system"""
    
    def __init__(self, data_dir: str =  r"C:\Users\likhi\Downloads\google_apps"):
        self.env = MockAndroidEnv()
        self.qa_system = VideoMultiAgentQA(self.env)
        self.video_loader = AndroidInWildVideoLoader(data_dir)
        self.prompt_generator = VideoTaskPromptGenerator()
        self.comparator = VideoGroundTruthComparator()
    
    def evaluate_video(self, video: AndroidInWildVideo) -> VideoComparisonResult:
        """Evaluate single video"""
        
        print(f"\n--- Evaluating Video: {video.episode_id} ---")
        print(f"Real goal: {video.goal_info}")
        print(f"Real actions: {len(video.actions)}")
        print(f"Video frames: {len(video.frames)}")
        
        # Step 1: Generate task prompt from video
        task_prompt = self.prompt_generator.generate_task_prompt(video)
        print(f"Generated prompt: '{task_prompt}'")
        
        # Step 2: Run multi-agent system
        agent_trace = self.qa_system.run_test(task_prompt)
        
        # Step 3: Compare video traces
        comparison = self.comparator.compare_video_traces(agent_trace, video)
        
        # Step 4: Calculate scores
        accuracy_score = self._calculate_accuracy(comparison, agent_trace)
        robustness_score = self._calculate_robustness(agent_trace, comparison)
        generalization_score = self._calculate_generalization(video, agent_trace, comparison)
        
        print(f"Results: Accuracy={accuracy_score:.2f}, Robustness={robustness_score:.2f}, Generalization={generalization_score:.2f}")
        
        return VideoComparisonResult(
            episode_id=video.episode_id,
            task_prompt=task_prompt,
            real_goal=video.goal_info,
            accuracy_score=accuracy_score,
            robustness_score=robustness_score,
            generalization_score=generalization_score,
            agent_trace=agent_trace,
            ground_truth_actions=video.actions,
            detailed_analysis=comparison
        )
    
    def _calculate_accuracy(self, comparison: Dict, agent_trace: AgentVideoTrace) -> float:
        """Calculate accuracy score"""
        action_sim = comparison["action_similarity"]
        visual_sim = comparison["visual_similarity"]
        goal_achievement = 1.0 if comparison["goal_achievement"] else 0.0
        efficiency = comparison["efficiency_score"]
        
        # Weighted combination with video-specific metrics
        accuracy = (action_sim * 0.25 + visual_sim * 0.25 + goal_achievement * 0.3 + efficiency * 0.2)
        return min(1.0, accuracy)
    
    def _calculate_robustness(self, agent_trace: AgentVideoTrace, comparison: Dict) -> float:
        """Calculate robustness score"""
        success_factor = 1.0 if agent_trace.success else 0.3
        efficiency_factor = comparison["efficiency_score"]
        navigation_factor = comparison["navigation_similarity"]
        
        robustness = (success_factor * 0.5 + efficiency_factor * 0.25 + navigation_factor * 0.25)
        return min(1.0, robustness)
    
    def _calculate_generalization(self, video: AndroidInWildVideo, agent_trace: AgentVideoTrace, comparison: Dict) -> float:
        """Calculate generalization score"""
        complexity = video.metadata.get("complexity", "medium")
        complexity_factor = {"simple": 0.3, "medium": 0.6, "high": 0.9}.get(complexity, 0.6)
        
        adaptability = comparison["action_similarity"]
        visual_adaptability = comparison["visual_similarity"]
        
        # Better generalization if agent adapts well to task complexity
        complexity_handling = 1.0 - abs(complexity_factor - 0.6)
        
        generalization = (adaptability * 0.4 + visual_adaptability * 0.3 + complexity_handling * 0.3)
        return min(1.0, generalization)
    
    def run_evaluation(self, num_videos: int = 5) -> Dict[str, Any]:
        """Run evaluation on multiple videos"""
        
        print(f" ANDROID IN THE WILD VIDEO EVALUATION SYSTEM")
        print(f"Testing {num_videos} video episodes")
        print("="*60)
        
        # Load videos
        videos = self.video_loader.load_videos(num_videos)
        
        if not videos:
            print(" No videos loaded! Please check your data directory or network connection.")
            return {}
        
        results = []
        total_accuracy = 0
        total_robustness = 0
        total_generalization = 0
        
        for video in videos:
            result = self.evaluate_video(video)
            results.append(result)
            
            total_accuracy += result.accuracy_score
            total_robustness += result.robustness_score
            total_generalization += result.generalization_score
        
        # Calculate overall metrics
        num_videos = len(results)
        avg_accuracy = total_accuracy / num_videos
        avg_robustness = total_robustness / num_videos
        avg_generalization = total_generalization / num_videos
        overall_score = (avg_accuracy + avg_robustness + avg_generalization) / 3
        
        success_rate = sum(1 for r in results if r.agent_trace.success) / num_videos
        
        evaluation_summary = {
            "system_info": {
                "environment": "MockAndroidEnv with Video Analysis",
                "videos_tested": num_videos,
                "timestamp": time.time()
            },
            "scores": {
                "accuracy": avg_accuracy,
                "robustness": avg_robustness,
                "generalization": avg_generalization,
                "overall": overall_score,
                "success_rate": success_rate
            },
            "detailed_results": results,
            "analysis": self._analyze_video_results(results)
        }
        
        self._display_video_results(evaluation_summary)
        
        return evaluation_summary
    
    def _analyze_video_results(self, results: List[VideoComparisonResult]) -> Dict[str, Any]:
        """Analyze video evaluation results"""
        
        # Task type performance
        task_types = {}
        complexity_analysis = {}
        
        for result in results:
            # Task type analysis
            task_type = self._categorize_task(result.task_prompt)
            if task_type not in task_types:
                task_types[task_type] = []
            task_types[task_type].append(result)
            
            # Complexity analysis
            complexity = result.detailed_analysis.get("complexity", "unknown")
            if complexity not in complexity_analysis:
                complexity_analysis[complexity] = []
            complexity_analysis[complexity].append(result)
        
        task_performance = {}
        for task_type, task_results in task_types.items():
            avg_accuracy = sum(r.accuracy_score for r in task_results) / len(task_results)
            avg_visual_sim = sum(r.detailed_analysis.get("visual_similarity", 0) for r in task_results) / len(task_results)
            success_rate = sum(1 for r in task_results if r.agent_trace.success) / len(task_results)
            
            task_performance[task_type] = {
                "count": len(task_results),
                "avg_accuracy": avg_accuracy,
                "avg_visual_similarity": avg_visual_sim,
                "success_rate": success_rate
            }
        
        return {
            "task_performance": task_performance,
            "complexity_analysis": complexity_analysis,
            "avg_agent_steps": sum(len(r.agent_trace.steps) for r in results) / len(results),
            "avg_real_steps": sum(len(r.ground_truth_actions) for r in results) / len(results),
            "avg_visual_similarity": sum(r.detailed_analysis.get("visual_similarity", 0) for r in results) / len(results),
            "avg_navigation_similarity": sum(r.detailed_analysis.get("navigation_similarity", 0) for r in results) / len(results)
        }
    
    def _categorize_task(self, task_prompt: str) -> str:
        """Categorize task for analysis"""
        prompt_lower = task_prompt.lower()
        
        if 'wifi' in prompt_lower:
            return "WiFi"
        elif 'bluetooth' in prompt_lower:
            return "Bluetooth"
        elif 'alarm' in prompt_lower:
            return "Clock/Alarm"
        elif 'location' in prompt_lower:
            return "Location"
        elif 'version' in prompt_lower:
            return "System_Info"
        else:
            return "Other"
    
    def _display_video_results(self, evaluation_summary: Dict[str, Any]):
        """Display video evaluation results"""
        
        scores = evaluation_summary["scores"]
        analysis = evaluation_summary["analysis"]
        
        print(f"\n{'='*60}")
        print(f" VIDEO EVALUATION RESULTS")
        print(f"{'='*60}")
        
        print(f" Overall Scores:")
        print(f"   Accuracy: {scores['accuracy']:.2%}")
        print(f"   Robustness: {scores['robustness']:.2%}")
        print(f"   Generalization: {scores['generalization']:.2%}")
        print(f"   Overall: {scores['overall']:.2%}")
        print(f"   Success Rate: {scores['success_rate']:.2%}")
        
        print(f"\n Video Analysis:")
        print(f"   Avg Agent Steps: {analysis['avg_agent_steps']:.1f}")
        print(f"   Avg Real Steps: {analysis['avg_real_steps']:.1f}")
        print(f"   Avg Visual Similarity: {analysis['avg_visual_similarity']:.2%}")
        print(f"   Avg Navigation Similarity: {analysis['avg_navigation_similarity']:.2%}")
        
        print(f"\n Task Type Performance:")
        for task_type, perf in analysis["task_performance"].items():
            print(f"   {task_type}: {perf['success_rate']:.1%} success, {perf['avg_visual_similarity']:.1%} visual sim ({perf['count']} tests)")
        
        if scores['overall'] >= 0.8:
            print(f"\n EXCELLENT: System performs excellently on Android in the Wild videos!")
        elif scores['overall'] >= 0.6:
            print(f"\n GOOD: System shows solid video performance with room for improvement")
        else:
            print(f"\n NEEDS WORK: System requires optimization for video tasks")

# Demo and testing functions
def run_video_evaluation_demo():
    """Run the complete video evaluation demo"""
    
    print(" ANDROID IN THE WILD VIDEO EVALUATION DEMO")
    print("="*50)
    
    # Initialize evaluation system
    evaluator = AndroidVideoEvaluationSystem()
    
    # Run evaluation on 5 videos
    results = evaluator.run_evaluation(num_videos=3)
    
    return results

def demo_single_video():
    """Demonstrate single video evaluation with REAL data"""
    
    print(" SINGLE VIDEO DEMO")
    print("="*25)
    
    # Load one real video from your dataset
    video_loader = AndroidInWildVideoLoader(r"C:\Users\likhi\Downloads\google_apps")
    videos = video_loader.load_videos(1)  # Load 1 real video
    
    if videos:
        video = videos[0]
        evaluator = AndroidVideoEvaluationSystem(r"C:\Users\likhi\Downloads\google_apps")
        result = evaluator.evaluate_video(video)
        
        print(f"\n Single REAL Video Results:")
        print(f"Video Goal: '{result.real_goal}'")
        print(f"Task Prompt: '{result.task_prompt}'")
        print(f"Agent Success: {result.agent_trace.success}")
        print(f"Agent Steps: {len(result.agent_trace.steps)}")
        print(f"Real Actions: {len(result.ground_truth_actions)}")
        print(f"Accuracy: {result.accuracy_score:.2%}")
        print(f"Visual Similarity: {result.detailed_analysis.get('visual_similarity', 0):.2%}")
        print(f"Navigation Similarity: {result.detailed_analysis.get('navigation_similarity', 0):.2%}")
        
        return result
    else:
        print(" Could not load real video from dataset")
        return None
def save_video_evaluation_results(results: Dict[str, Any], filename: str = None):
    """Save video evaluation results to JSON (fixed serialization)"""
    
    if filename is None:
        filename = f"video_evaluation_results_{int(time.time())}.json"
    
    # Convert results to JSON-serializable format
    json_results = {
        "system_info": results["system_info"],
        "scores": results["scores"],
        "analysis": {
            "task_performance": results["analysis"]["task_performance"],
            "avg_agent_steps": results["analysis"]["avg_agent_steps"],
            "avg_real_steps": results["analysis"]["avg_real_steps"],
            "avg_visual_similarity": results["analysis"]["avg_visual_similarity"],
            "avg_navigation_similarity": results["analysis"]["avg_navigation_similarity"]
        },
        "summary": {
            "total_videos": len(results.get("detailed_results", [])),
            "overall_performance": results["scores"]["overall"],
            "success_rate": results["scores"]["success_rate"],
            "methodology": "Real Android in the Wild Video Evaluation",
            "real_tasks_evaluated": [
                result.real_goal for result in results.get("detailed_results", [])
            ]
        }
    }
    
    with open(filename, 'w') as f:
        json.dump(json_results, f, indent=2)
    
    print(f" Results saved to: {filename}")

def analyze_video_comparison_methodology():
    """Analyze the video comparison methodology"""
    
    print(" VIDEO COMPARISON METHODOLOGY ANALYSIS")
    print("="*50)
    
    print(" What We're Measuring:")
    print("   1. Video Loading: Real Android in the Wild videos â†’ Processed episodes")
    print("   2. Task Prompt Generation: Video analysis â†’ Clear task instructions")
    print("   3. Agent Video Execution: Multi-agent system â†’ Video trace with screenshots")
    print("   4. Video Trace Comparison: Agent video vs Ground truth video")
    print("   5. Multi-dimensional Scoring: Accuracy + Robustness + Generalization")
    
    print("\n Enhanced Scoring Methodology:")
    print("    Accuracy (0-1):")
    print("     â€¢ Action similarity (25%): LCS of action sequences")
    print("     â€¢ Visual similarity (25%): Screenshot/screen state comparison")
    print("     â€¢ Goal achievement (30%): Task completion success")
    print("     â€¢ Efficiency (20%): Real steps / Agent steps")
    
    print("    Robustness (0-1):")
    print("     â€¢ Execution success (50%): Did agent complete task?")
    print("     â€¢ Efficiency factor (25%): How efficient was execution?")
    print("     â€¢ Navigation similarity (25%): UI navigation pattern matching")
    
    print("    Generalization (0-1):")
    print("     â€¢ Action adaptability (40%): How well did agent match real action patterns?")
    print("     â€¢ Visual adaptability (30%): How well did agent match visual progression?")
    print("     â€¢ Complexity handling (30%): Appropriate response to task complexity")
    
    print("\n Video-Specific Comparison Features:")
    print("    Action sequence similarity (enhanced LCS algorithm)")
    print("    Visual progression comparison (screen state sequences)")
    print("    Navigation pattern analysis (app switching, UI flow)")
    print("    Goal achievement verification")
    print("    Execution efficiency measurement")
    print("    Task complexity adaptation assessment")
    
    print("\n Video Processing Pipeline:")
    print("    Video Loading:")
    print("     â€¢ TensorFlow Datasets (Android in the Wild)")
    print("     â€¢ Local video files (.mp4, .avi)")
    print("     â€¢ Realistic mock videos based on research patterns")
    
    print("    Task Extraction:")
    print("     â€¢ Goal text processing from metadata")
    print("     â€¢ Video frame analysis for task inference")
    print("     â€¢ Action sequence analysis for task understanding")
    
    print("    Comparison Metrics:")
    print("     â€¢ Action type mapping (DUAL_POINT â†’ tap, etc.)")
    print("     â€¢ Coordinate region mapping for spatial comparison")
    print("     â€¢ Screen state transition analysis")
    print("     â€¢ Navigation flow pattern matching")
    
    print("\n Key Advantages:")
    print("   â€¢ Real Android in the Wild data integration")
    print("   â€¢ Video-aware evaluation metrics")
  

def test_video_loading():
    """Test video loading capabilities"""
    
    print(" VIDEO LOADING TEST")
    print("="*25)
    
    loader = AndroidInWildVideoLoader(r"C:\Users\likhi\Downloads\google_apps")
    
    print("Testing real dataset loading...")
    
    # Test REAL video loading
    try:
        real_videos = loader.load_videos(3)
        
        if real_videos:
            print(f"\n Loaded {len(real_videos)} REAL videos from dataset:")
            for i, video in enumerate(real_videos):
                print(f"   Video {i+1}: '{video.goal_info}' ({len(video.actions)} actions, {len(video.frames)} frames)")
                print(f"     Source: {video.metadata.get('source', 'unknown')}")
            return real_videos
        else:
            print("\n No real videos loaded from dataset")
            
    except Exception as e:
        print(f"\n Error loading real videos: {e}")
        import traceback
        traceback.print_exc()
    
    return []

def demonstrate_video_action_comparison():
    """Demonstrate how video action comparison works"""
    
    print(" VIDEO ACTION COMPARISON DEMO")
    print("="*40)
    
    # Create example video actions
    real_video_actions = [
        {"action_type": ActionType.DUAL_POINT, "touch_point": [540, 400], "timestamp": 0.0},  # Settings
        {"action_type": ActionType.DUAL_POINT, "touch_point": [450, 300], "timestamp": 1.2},  # WiFi
        {"action_type": ActionType.DUAL_POINT, "touch_point": [950, 350], "timestamp": 2.1}   # Toggle
    ]
    
    # Agent actions (good match)
    agent_actions_good = [
        {"action_type": "key", "key": "KEYCODE_SETTINGS"},
        {"action_type": "tap", "coordinate": [450, 450]},  # WiFi area
        {"action_type": "tap", "coordinate": [950, 400]}   # Toggle area
    ]
    
    # Agent actions (poor match)
    agent_actions_poor = [
        {"action_type": "tap", "coordinate": [300, 300]},  # Wrong location
        {"action_type": "scroll", "coordinate": [540, 800]},  # Unnecessary scroll
        {"action_type": "tap", "coordinate": [200, 600]},  # Wrong target
        {"action_type": "tap", "coordinate": [950, 400]}   # Finally correct
    ]
    
    comparator = VideoGroundTruthComparator()
    
    # Test good match
    good_patterns = [comparator._extract_action_pattern(a) for a in agent_actions_good]
    real_patterns = [comparator._extract_real_action_pattern(a) for a in real_video_actions]
    good_similarity = comparator._lcs_similarity(good_patterns, real_patterns)
    
    print(f" Good Agent Match:")
    print(f"   Real video: {real_patterns}")
    print(f"   Agent: {good_patterns}")
    print(f"   Similarity: {good_similarity:.2%}")
    
    # Test poor match
    poor_patterns = [comparator._extract_action_pattern(a) for a in agent_actions_poor]
    poor_similarity = comparator._lcs_similarity(poor_patterns, real_patterns)
    
    print(f"\n Poor Agent Match:")
    print(f"   Real video: {real_patterns}")
    print(f"   Agent: {poor_patterns}")
    print(f"   Similarity: {poor_similarity:.2%}")
    
    print(f"\n Video Action Analysis:")
    print(f"   â€¢ Good match: Efficient path with correct targets")
    print(f"   â€¢ Poor match: Extra actions, wrong targets, inefficient navigation")
    print(f"   â€¢ Region mapping: Coordinates mapped to screen regions for comparison")

def benchmark_video_evaluation_speed():
    """Benchmark video evaluation system speed"""
    
    print(" VIDEO EVALUATION SPEED BENCHMARK")
    print("="*40)
    
    # Load real videos from your dataset
    loader = AndroidInWildVideoLoader(r"C:\Users\likhi\Downloads\google_apps")
    videos = loader.load_videos(5)  # Load 5 real videos
    
    if not videos:
        print(" Could not load videos for benchmark")
        return
    
    print(f" Benchmarking with {len(videos)} REAL videos from dataset")
    
    # Time the evaluation
    start_time = time.time()
    
    evaluator = AndroidVideoEvaluationSystem(r"C:\Users\likhi\Downloads\google_apps")
    results = evaluator.run_evaluation(len(videos))
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"\nâš¡ Video Performance Metrics:")
    print(f"   Videos evaluated: {len(videos)}")
    print(f"   Total time: {duration:.2f} seconds")
    print(f"   Time per video: {duration/len(videos):.2f} seconds")
    print(f"   Videos per minute: {len(videos)/(duration/60):.1f}")
    print(f"   Total actions processed: {sum(len(v.actions) for v in videos)}")
    print(f"   Total frames analyzed: {sum(len(v.frames) for v in videos)}")
    
    print(f"\n Video Processing Speed Benefits:")
    print(f"   â€¢ No actual video decoding delays")
    print(f"   â€¢ No computer vision processing time")
    print(f"   â€¢ No network download time for videos")
    print(f"   â€¢ Fast action sequence comparison")
    print(f"   â€¢ Efficient screen state analysis")
    print(f"   â€¢ Scalable to large video datasets")
    
    print(f"\n Real Dataset Performance:")
    print(f"   â€¢ Real goals processed: {[v.goal_info[:30] + '...' for v in videos]}")
    print(f"   â€¢ Real action sequences compared")
    print(f"   â€¢ Real video frames analyzed")

def demonstrate_task_prompt_generation_from_video():
    """Demonstrate task prompt generation from video analysis"""
    
    print(" VIDEO TASK PROMPT GENERATION DEMO")
    print("="*45)
    
    loader = AndroidInWildVideoLoader()
    generator = VideoTaskPromptGenerator()
    
    # Create various realistic videos
    test_scenarios = [
        "turn on wifi",
        "set alarm for 7 AM", 
        "enable bluetooth and connect device",
        "turn off improve location accuracy",
        "check android version"
    ]
    
    print("Video Goal â†’ Generated Task Prompt:")
    print("-" * 50)
    
    for goal in test_scenarios:
        # Create mock video with this goal
        mock_video = AndroidInWildVideo(
            episode_id="test",
            goal_info=goal,
            frames=[],
            actions=[],
            metadata={"complexity": "medium"},
            video_path=""
        )
        
        prompt = generator.generate_task_prompt(mock_video)
        print(f"'{goal}' â†’ '{prompt}'")
    
    print(f"\n Video-aware task prompt generation:")
    print(f"   â€¢ Analyzes video goal information")
    print(f"   â€¢ Considers action complexity")
    print(f"   â€¢ Generates specific, actionable prompts")
    print(f"   â€¢ Handles various Android task types")

def create_integration_guide():
    """Create integration guide for real Android in the Wild data"""
    
    print(" ANDROID IN THE WILD INTEGRATION GUIDE")
    print("="*50)
    
    print(" Integration Steps:")
    print("1. Install TensorFlow Datasets:")
    print("   pip install tensorflow-datasets")
    print("   pip install android-in-the-wild")
    
    print("\n2. Load Real Data:")
    print("   # Replace mock data loading with:")
    print("   import tensorflow_datasets as tfds")
    print("   ds = tfds.load('android_in_the_wild', split='train')")
    
    print("\n3. Process Video Episodes:")
    print("   # Extract frames, actions, and goals from TFRecord format")
    print("   # Map action types to our ActionType enum")
    print("   # Convert coordinates to our coordinate system")
    
    print("\n4. Replace Mock MultiAgent with Real System:")
    print("   # Integrate your actual multi-agent QA system")
    print("   # Replace MockMultiAgentQA class")
    print("   # Ensure it returns AgentVideoTrace format")
    
    print("\n5. Enhance Video Analysis:")
    print("   # Add computer vision for UI element detection")
    print("   # Implement screenshot comparison algorithms")
    print("   # Add OCR for text recognition in videos")
    
    print("\n Data Format Mapping:")
    print("   Real Format â†’ Our Format:")
    print("   â€¢ TFRecord episodes â†’ AndroidInWildVideo")
    print("   â€¢ Video frames â†’ VideoFrame objects")
    print("   â€¢ DUAL_POINT actions â†’ tap actions")
    print("   â€¢ Touch coordinates â†’ screen regions")
    print("   â€¢ Goal text â†’ task prompts")
    
    print("\n Evaluation Pipeline:")
    print("   1. Load 3-5 real Android in the Wild videos")
    print("   2. Extract task prompts from video analysis")
    print("   3. Run multi-agent system in AndroidEnv/android_world")
    print("   4. Compare agent video trace vs ground truth video")
    print("   5. Score accuracy, robustness, generalization")
    
    print("\n Current System Capabilities:")
    print("    Video loading and processing framework")
    print("    Task prompt generation from video goals")
    print("    Multi-agent execution with video capture")
    print("    Action sequence comparison")
    print("    Visual progression tracking")
    print("    Multi-dimensional scoring")
    print("    Comprehensive evaluation metrics")

if __name__ == "__main__":
    try:
        print(" ANDROID IN THE WILD VIDEO EVALUATION SYSTEM")
        print("Complete video-based evaluation with real data integration")
        print("="*70)
        
        # 1. Test video loading
        print("\n1. Video Loading Test:")
        test_videos = test_video_loading()
        
        # 2. Demo single video evaluation
        print("\n2. Single Video Evaluation:")
        single_result = demo_single_video()
        
        # 3. Demonstrate task prompt generation
        print("\n3. Task Prompt Generation from Videos:")
        demonstrate_task_prompt_generation_from_video()
        
        # 4. Demonstrate video action comparison
        print("\n4. Video Action Comparison:")
        demonstrate_video_action_comparison()
        
        # 5. Analyze methodology
        print("\n5. Video Methodology Analysis:")
        analyze_video_comparison_methodology()
        
        # 6. Integration guide
        print("\n6. Real Data Integration Guide:")
        create_integration_guide()
        
        # 7. Benchmark speed
        print("\n7. Video Evaluation Speed Benchmark:")
        benchmark_video_evaluation_speed()
        
        # 8. Run full video evaluation
        print("\n8. Full Video Evaluation:")
        evaluation_results = run_video_evaluation_demo()
        
        # 9. Save results
        if evaluation_results:
            save_video_evaluation_results(evaluation_results)
        
        print(f"\n ANDROID IN THE WILD VIDEO EVALUATION COMPLETE!")
        if evaluation_results:
            print(f" {evaluation_results['system_info']['videos_tested']} videos evaluated")
            print(f" Overall Performance: {evaluation_results['scores']['overall']:.1%}")
            print(f" Success Rate: {evaluation_results['scores']['success_rate']:.1%}")
            print(f" Avg Visual Similarity: {evaluation_results['analysis']['avg_visual_similarity']:.1%}")
        
        print(f"\n Key Video Evaluation Features:")
        print(f"    Real Android in the Wild video integration")
        print(f"    Video-aware task prompt generation")
        print(f"    Multi-agent execution with video capture")
        print(f"    Visual progression and action sequence comparison")
        print(f"    Enhanced scoring with video-specific metrics")
        print(f"    MockAndroid environment (no device complexity)")
        print(f"    Fast, scalable video evaluation")
        
        print(f"\n Ready for Real Android in the Wild Integration:")
        print(f"   â€¢ Load your real video TFRecord files")
        print(f"   â€¢ Replace MockMultiAgentQA with your actual system")
        print(f"   â€¢ Add computer vision for enhanced video analysis")
        print(f"   â€¢ Scale to hundreds of videos")
        print(f"   â€¢ Get comprehensive video-based performance metrics")
        
        print(f"\n Next Steps:")
        print(f"   1. Install TensorFlow Datasets for real data loading")
        print(f"   2. Integrate your multi-agent QA system")
        print(f"   3. Add computer vision for UI element detection")
        print(f"   4. Run evaluation on real Android in the Wild videos")
        print(f"   5. Analyze results and optimize your system")
        
    except Exception as e:
        print(f" Error during video evaluation: {e}")
        import traceback
        traceback.print_exc()